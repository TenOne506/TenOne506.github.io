---
title: 反向求导小记
createTime: 2026/01/31 17:26:47
permalink: /searchengine/y7y843s1/
---

当然可以！以下是将你提供的内容整理为规范的 Markdown 格式，**从三级标题（`###`）开始**，并保留所有数学公式、列表结构和逻辑层次，适用于博客、笔记或文档系统（如 Hugo、Obsidian、Typora 等）：

---

### 一、核心工具：导数、梯度、雅可比矩阵与链式法则

#### 1. 基础定义

- **标量函数导数**：单变量函数 $f: \mathbb{R} \to \mathbb{R}$ 的导数为局部线性逼近的斜率：
  $$
  f(x+h) \approx f(x) + f'(x)h
  $$

- **梯度（向量→标量）**：函数 $f: \mathbb{R}^n \to \mathbb{R}$ 的梯度是偏导数的列向量，反映输入对输出的敏感度：
  $$
  \nabla_x f = \left( \frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right)^T
  $$
  满足 $f(x+h) \approx f(x) + \langle \nabla_x f, h \rangle$（$h$ 为小扰动）。

- **雅可比矩阵（向量→向量）**：函数 $f: \mathbb{R}^n \to \mathbb{R}^m$ 的雅可比为 $m \times n$ 矩阵，元素 $(\frac{\partial f}{\partial x})_{ij} = \frac{\partial f_i}{\partial x_j}$，是梯度的推广（$m=1$ 时雅可比为梯度转置）。

- **链式法则**：复合函数 $g(f(x))$ 的导数为雅可比矩阵相乘：
  $$
  \frac{\partial g}{\partial x} = \frac{\partial g}{\partial f} \cdot \frac{\partial f}{\partial x}
  $$

#### 2. 关键结论

- 微分是线性逼近，梯度是微分的向量表示，雅可比是梯度的推广；
- 链式法则通过雅可比矩阵乘法高效计算复合函数导数，避免符号混淆；
- 多变量函数求导优先用雅可比 + 链式法则，确保梯度形状正确。

---

### 二、神经网络中的向量化梯度计算

#### 1. 核心思路

用雅可比矩阵 + 链式法则实现向量化梯度计算，避免单参数求导的低效性，给出常见操作的梯度恒等式。

#### 2. 实用恒等式
$x$在后为列向量

$x$在前为行向量

| 编号 | 场景                                                                  | 公式                                                                                    | 说明                                                  |
| ---- | --------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ----------------------------------------------------- |
| (1)  | $z = Wx$，对 $x$ 求导                                                 | $\displaystyle \frac{\partial z}{\partial x} = W$                                       | $W \in \mathbb{R}^{n \times m}$，$z \in \mathbb{R}^n$ |
| (2)  | $z = xW$，对 $x$ 求导                                                 | $\displaystyle \frac{\partial z}{\partial x} = W^T$                                     | $x$ 为行向量，$z$ 为行向量                            |
| (3)  | $z = x$，对 $x$ 求导                                                  | $\displaystyle \frac{\partial z}{\partial x} = I$                                       | 单位矩阵                                              |
| (4)  | 向量逐元素函数 $z = f(x)$                                             | $\displaystyle \frac{\partial z}{\partial x} = \text{diag}(f'(x))$ 或记作 $\circ f'(x)$ | 元素乘                                                |
| (5)  | $z = Wx$，求 $\frac{\partial J}{\partial W}$                          | $\displaystyle \frac{\partial J}{\partial W} = \delta^T x^T$                            | $\delta = \frac{\partial J}{\partial z}$              |
| (6)  | $z = xW$，求 $\frac{\partial J}{\partial W}$                          | $\displaystyle \frac{\partial J}{\partial W} = x^T \delta$                              | 同上                                                  |
| (7)  | 交叉熵损失 $J = CE(y, \hat{y})$（$\hat{y} = \text{softmax}(\theta)$） | $\displaystyle \frac{\partial J}{\partial \theta} = \hat{y} - y$                        | 列向量时为 $(\hat{y} - y)^T$                          |

#### 3. 梯度布局约定

- **Jacobian 形式**：便于链式法则（矩阵相乘）；
- **SGD 常用约定**：梯度形状与参数一致（如 $W$ 为 $n \times m$，则 $\frac{\partial J}{\partial W}$ 也为 $n \times m$），便于直接更新参数（`param -= lr * grad`）；
- 若 Jacobian 结果与参数形状不符，需转置（如列向量参数的梯度为行向量时需转置）。

#### 4. 示例：单层神经网络

##### （1）前向过程
$$
\begin{aligned}
x &= \text{input} \quad (D_x \times 1) \\
z &= Wx + b_1 \quad (W: D_h \times D_x,\ b_1: D_h \times 1) \\
h &= \text{ReLU}(z) \quad (\text{逐元素激活}) \\
\theta &= Uh + b_2 \quad (U: N_c \times D_h,\ b_2: N_c \times 1) \\
\hat{y} &= \text{softmax}(\theta) \quad (\text{分类概率}) \\
J &= CE(y, \hat{y}) \quad (\text{交叉熵损失})
\end{aligned}
$$

##### （2）反向传播与误差信号

定义误差信号（避免重复计算）：

- $\delta_1 = \frac{\partial J}{\partial \theta} = (\hat{y} - y)^T$（用恒等式 7）；
- $\delta_2 = \frac{\partial J}{\partial z} = \delta_1 U \circ \text{ReLU}'(z)$（链式法则 + 恒等式 4，$\text{ReLU}'(z) = \text{sgn}(\text{ReLU}(z))$）。

##### （3）各参数梯度（形状匹配约定）
$$
\begin{aligned}
\frac{\partial J}{\partial U} &= \delta_1^T h^T \quad (\text{恒等式 5}) \\
\frac{\partial J}{\partial b_2} &= \delta_1^T \quad (\text{恒等式 3 + 转置}) \\
\frac{\partial J}{\partial W} &= \delta_2^T x^T \quad (\text{恒等式 5}) \\
\frac{\partial J}{\partial b_1} &= \delta_2^T \quad (\text{恒等式 3 + 转置}) \\
\frac{\partial J}{\partial x} &= (\delta_2 W)^T \quad (\text{链式法则})
\end{aligned}
$$

#### 5. 结论

- 向量化梯度核心是雅可比 + 链式法则，恒等式大幅简化推导；
- 误差信号（$\delta_1, \delta_2$）避免重复计算，提升效率；
- 梯度布局需匹配参数形状，确保实现正确。

---

### 三、反向传播的直观理解与实践

#### 1. 核心思路

反向传播是局部梯度 + 链式法则的递归应用，将函数拆分为“门”（可微函数），计算每个门的局部梯度，再反向传播误差。

#### 2. 关键概念与例子

- **简单门（局部梯度易推导）**：
  - 乘法门：$f(x,y)=xy$，$\frac{\partial f}{\partial x}=y$，$\frac{\partial f}{\partial y}=x$；
  - 加法门：$f(x,y)=x+y$，$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=1$；
  - 最大门：$f(x,y)=\max(x,y)$，$\frac{\partial f}{\partial x}=1(x\geq y)$，$\frac{\partial f}{\partial y}=1(y\geq x)$。

- **复合表达式例子**：$f(x,y,z)=(x+y)z$，拆分为 $q=x+y$ 和 $f=qz$：
  - 前向：$q=3$，$f=-12$；
  - 反向：$\frac{\partial f}{\partial z}=q=3$，$\frac{\partial f}{\partial q}=z=-4$，$\frac{\partial q}{\partial x}=1$，$\frac{\partial q}{\partial y}=1$；
  - 链式法则：$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q} \cdot \frac{\partial q}{\partial x}=-4$，$\frac{\partial f}{\partial y}=-4$。

#### 3. 实践技巧

- **分阶段计算**：将前向过程拆分为中间变量（如 `dot = w·x`），反向时缓存中间结果，避免重复计算；
- **梯度累加**：若变量在多个路径中出现（如 $x$ 参与多个运算），用 `+=` 累加梯度（多变量链式法则）；
- **维度分析**：向量化运算中，用输入输出维度推导梯度形状（如 $W \in \mathbb{R}^{5×10}$，$X \in \mathbb{R}^{10×3}$，则 $\frac{\partial J}{\partial W} = dD \cdot X^T$，$dD \in \mathbb{R}^{5×3}$）。

#### 4. 结论

- 反向传播是局部过程：每个门独立计算局部梯度，再通过链式法则传递误差；
- 分阶段实现是关键：拆分复杂函数为简单门，缓存中间变量，降低复杂度；
- 维度分析可快速推导向量化梯度，避免记忆公式。

---

### 四、张量运算的梯度与反向传播

#### 1. 核心问题

深度学习中大量张量运算（如图像 3D 张量），显式构造雅可比矩阵内存不可行（如 $N=64, M=D=4096$ 时，雅可比矩阵需 256GB 内存）。

#### 2. 解决方法：避免显式雅可比

通过推导梯度乘积公式，直接计算 $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$（无需构造 $\frac{\partial y}{\partial x}$）。

#### 3. 例子：线性层 $z = xw$（$x \in \mathbb{R}^{1×2}$，$w \in \mathbb{R}^{2×3}$）

- **前向**：$z = xw = [x_1w_{11}+x_2w_{21}, x_1w_{12}+x_2w_{22}, x_1w_{13}+x_2w_{23}]$；
- **反向**：
  $$
  \begin{aligned}
  \frac{\partial L}{\partial x_{11}} &= \frac{\partial L}{\partial z_1}w_{11} + \frac{\partial L}{\partial z_2}w_{12} + \frac{\partial L}{\partial z_3}w_{13}, \\
  \frac{\partial L}{\partial x_{12}} &= \frac{\partial L}{\partial z_1}w_{21} + \frac{\partial L}{\partial z_2}w_{22} + \frac{\partial L}{\partial z_3}w_{23};
  \end{aligned}
  $$
- **向量化结果**：$\displaystyle \frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot w^T$。

<!-- > ⚠️ 注：原文写为 $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot x^T$，此处应为 $w^T$，已修正。 -->

#### 4. 结论

- 显式雅可比矩阵在张量运算中不可行，需推导梯度乘积公式；
- 小例子推导 + 推广到向量化形式是有效方法；
- 避免存储巨型矩阵，提升计算可行性与效率。

---

### 五、神经网络结构与激活函数

#### 1. 核心结构：全连接层

全连接层是神经网络的基础，前向过程为矩阵乘法 + 偏置 + 激活函数：
$$
\text{output} = f(W \cdot \text{input} + b)
$$
其中 $W$ 为权重矩阵，$b$ 为偏置，$f$ 为激活函数。

#### 2. 激活函数对比

| 激活函数   | 公式                                  | 优点                                        | 缺点                               | 推荐度   |
| ---------- | ------------------------------------- | ------------------------------------------- | ---------------------------------- | -------- |
| Sigmoid    | $\sigma(x) = \frac{1}{1+e^{-x}}$      | 输出 ∈ [0,1]，解释为 firing rate            | 饱和区梯度消失；输出非零中心       | ❌ 不推荐 |
| Tanh       | $\tanh(x) = 2\sigma(2x)-1$            | 输出 ∈ [-1,1]，零中心                       | 饱和区梯度消失                     | ⚠️ 次选   |
| ReLU       | $f(x) = \max(0, x)$                   | 收敛快（6 倍于 tanh）；计算简单；无饱和可能 | “死亡”（梯度为 0，神经元永久失效） | ✅ 首选   |
| Leaky ReLU | $f(x) = ax(x<0) + x(x≥0)$             | 解决 ReLU 死亡问题                          | 结果不稳定                         | ⚠️ 备选   |
| Maxout     | $f(x) = \max(w_1^Tx+b_1, w_2^Tx+b_2)$ | 无饱和、无死亡                              | 参数翻倍                           | ⚠️ 备选   |

#### 3. 网络规模与过拟合

- **误区**：小网络可防止过拟合？错！小网络更难训练（易陷入差局部极小值），大网络 + 正则化更好；
- **正则化方法**：权重衰减（L2）、Dropout、输入噪声；
- **结论**：用尽可能大的网络（计算预算内），用正则化控制过拟合。

#### 4. 结论

- 全连接层是神经网络的基础，前向过程为矩阵运算 + 激活；
- ReLU 是最优激活函数，避免 Sigmoid；
- 大网络 + 正则化优于小网络，正则化是控制过拟合的关键。

---

### 六、反向传播算法起源

#### 1. 核心思想

通过梯度下降调整网络权重，最小化输出误差，使隐藏单元学习任务的规律（创建有用特征）。

#### 2. 算法步骤

- **前向 pass**：计算每层单元状态（输入 → 输出）；
- **反向 pass**：从输出层到输入层传播误差，计算权重梯度：
  - 输出层误差：$\displaystyle \frac{\partial E}{\partial y_j} = y_j - d_j$（$E = \frac{1}{2}\sum_c \sum_j (y_{j,c} - d_{j,c})^2$）；
  - 隐藏层误差：$\displaystyle \frac{\partial E}{\partial y_i} = \sum_j \frac{\partial E}{\partial x_j} w_{ji}$（链式法则）；
  - 权重梯度：$\displaystyle \frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial x_j} y_i$。

#### 3. 权重更新

- **简单梯度下降**：$\Delta w = -\varepsilon \frac{\partial E}{\partial w}$；
- **动量法（加速）**：$\Delta w(t) = -\varepsilon \frac{\partial E}{\partial w(t)} + \alpha \Delta w(t-1)$（$\alpha$ 为动量系数）。

#### 4. 结论

- 反向传播是通用学习算法，可让隐藏单元学习有用特征；
- 缺点是可能陷入局部极小值，但实践中很少发生（加少量连接可缓解）；
- 是现代深度学习的基础，虽不 biologically plausible，但有效。

---

### 七、整体结论

- **理论基础**：导数、梯度、雅可比矩阵与链式法则是反向传播的核心，多变量函数求导优先用雅可比 + 链式法则；
- **实践关键**：向量化梯度（恒等式）、分阶段反向传播（缓存中间变量）、维度分析（推导梯度形状）；
- **结构设计**：全连接层 + ReLU 激活是基础，大网络 + 正则化（权重衰减、Dropout）优于小网络；
- **历史脉络**：反向传播算法（1986）是现代深度学习的基础，解决了隐藏单元的学习问题；
- **注意事项**：梯度布局需匹配参数形状，避免显式雅可比矩阵（张量运算），用动量法加速训练。



