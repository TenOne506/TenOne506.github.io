---
title: 向量化
createTime: 2025/04/30 20:26:03
permalink: /searchengine/s4gmkip2/
---

这里将会从向量化开始记录，虽然这些在之前NLP的相关学习中也都了解过，并知晓。但是还是打算自己重新记录整理一下。

### 让计算机理解文本

计算机中对文本的记录使用过编码来实现的，比如最常见的中文的编码方案，Unicode UTF-8的编码。变长编码（1-4字节），兼容ASCII，支持全球所有语言。中文字符通常占3字节。并且成为了互联网主导编码（98.2%网页使用），解决多语言同屏显示问题。

那为什么在机器学习中和深度学习中，文本的处理和表示方式会有所不同呢？

下面以UTF-8的编码格式说明。

首先是计算效率的问题，UTF-8是变长编码，导致文本向量化结果长度不一致，难以直接输入模型。而机器学习中需要固定纬度的输入。同时UTF-8编码的字符串需先解码为Unicode码点（如U+4E25），再转换为数值向量，额外步骤会增加预处理复杂度。

其次缺少了语义信息缺失，utf-8仅仅解决了字符存储的问题。无法捕捉单词或句子的语义关系（如同义词、上下文）。而NLP任务依赖语义建模，需通过词嵌入（如Word2Vec、BERT）将文本映射到低维稠密向量空间。

最后是高纬稀疏醒的问题，如果将每个Unicode码点直接作为特征，会生成高维稀疏向量（如Unicode包含百万级码点，Unicode的编码空间是21位，理论上可支持1,114,112个码点，实际文本中，单个文档或句子仅使用极少数码点（如中文常用字约几千个），导致向量中99%以上的位置为0。），导致维度灾难和模型效率低下。

### One-Hot编码

它将每个离散属性的每个类别创建一个新的二进制特征。对于每个样本，只有一个二进制特征为1，表示它属于对应的类别，其他特征为0。比如我们有三个类别老鼠，猫以及狗，那么他们对应的编码应该如下图所示


![picture1.1](/searchengine/one-hot.jpeg){ style="display: block; margin-left: auto; margin-right: auto;" }

### Word2Vec模型

---
Word2Vec 是一种经典的词嵌入（Word Embedding）模型，由 Google 团队在 2013 年提出。它通过神经网络将单词映射到低维稠密向量空间，使得语义相似的词在向量空间中距离相近。Word2Vec 的核心思想是“一个词的语义由其上下文决定”，即分布假说（Distributional Hypothesis）。


以下整理的 Word2Vec 模型详解:可以参考这篇[CSDN](https://blog.csdn.net/v_JULY_v/article/details/102708459)博客，写的很好。

---

**1. 模型架构**
Word2Vec 包含两种主要结构：

**(1) CBOW (Continuous Bag-of-Words)**
$$
h = \frac{1}{C} \sum_{i=1}^{C} \mathbf{W}^T \mathbf{x}_i
$$
$$
\hat{y} = \text{Softmax}(\mathbf{W'} h)
$$
• 变量说明：

  • $C$：上下文窗口大小  

  • $\mathbf{x}_i$：上下文词的 One-Hot 向量（维度 $|V|$）  

  • $\mathbf{W}$：输入层到隐藏层的权重矩阵（维度 $|V| \times d$）  

  • $\mathbf{W'}$：隐藏层到输出层的权重矩阵  


**(2) Skip-gram**
$$
h = \mathbf{W}^T \mathbf{x}
$$
$$
\hat{y}_c = \text{Softmax}(\mathbf{W'} h), \quad c \in \{1, \dots, C\}
$$
• 变量说明：

  • $\mathbf{x}$：中心词的 One-Hot 向量  


---

**2. 训练优化**
**(1) 负采样 (Negative Sampling)**
$$
\mathcal{L} = -\log \sigma(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I}) - \sum_{i=1}^k \log \sigma(-\mathbf{v}_{w_i}^T \mathbf{v}_{w_I})
$$
• 变量说明：

  • $\mathbf{v}_{w_O}$：目标词向量  

  • $\mathbf{v}_{w_I}$：输入词向量  

  • $k$：负样本数量（通常 5–20）  


**(2) 层次 Softmax (Hierarchical Softmax)**
这里层次化，出自于基于词频的哈夫曼树，讲$O(n)$的时间复杂度降到$O(log n)$
$$
p(w|w_I) = \prod_{j=1}^{L(w)-1} \sigma(\llbracket n(w,j+1) = \text{left-child}(n(w,j)) \rrbracket \cdot \mathbf{v}_{n(w,j)}^T \mathbf{v}_{w_I})
$$
• 变量说明：

  • $L(w)$：词 $w$ 的路径长度  

  • $n(w,j)$：路径上的第 $j$ 个节点  


---

**3. 模型特性**
**优点**
1. 语义相似性：
   $$
   \text{sim}(\text{"king"} - \text{"man"} + \text{"woman"}, \text{"queen"}) \approx 1
   $$
2. 高效训练：负采样和层次 Softmax 降低计算量。

**缺点**
1. 多义词问题：
   $$
   \mathbf{v}_{\text{bank}} \text{ 无法区分 "河岸" 和 "银行"}
   $$
2. OOV 问题：未登录词无向量表示。

---

**4. 应用示例**
**词向量计算（Python + Gensim）**
```python
from gensim.models import Word2Vec

# 训练模型
model = Word2Vec(
    sentences=corpus,      # 分词后的语料
    vector_size=100,       # 向量维度 $$ d $$
    window=5,              # 上下文窗口 $$ C $$
    min_count=1,           # 忽略低频词
    sg=1                   # 1=Skip-gram, 0=CBOW
)

# 获取词向量
v_computer = model.wv["computer"]
```

---



**核心公式总结**
1. Skip-gram 目标函数：
   $$
   \mathcal{L} = -\sum_{c=1}^C \log p(w_c | w_I)
   $$
2. 负采样损失：
   $$
   \mathcal{L} = -\log \sigma(\mathbf{v}_{w_O}^T \mathbf{v}_{w_I}) - \sum_{i=1}^k \log \sigma(-\mathbf{v}_{w_i}^T \mathbf{v}_{w_I})
   $$
3. 余弦相似度：
   $$
   \text{sim}(\mathbf{a}, \mathbf{b}) = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}
   $$
---

**总结**
| 特性 | Word2Vec | 改进模型（如 BERT） |
|------|---------|------------------|
| 训练方式 | 浅层神经网络 | 深度 Transformer |
| 上下文建模 | 局部窗口 | 全局自注意力 |
| 多义词处理 | ❌ 不支持 | ✅ 支持 |
| 计算效率 | ⚡ 高效 | 🐢 较慢 |
| 适用场景 | 小规模语料、快速训练 | 大规模预训练、复杂任务 |

Word2Vec 是 NLP 的里程碑模型，虽然已被 BERT 等取代，但其思想（如负采样、词嵌入）仍影响深远。

###  Bert模型

Bert可以参考这篇[CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/127411638)，和上文同一个作者，写的也很好。

---

**1. BERT 核心思想**

BERT 采用双向 Transformer 结构，通过预训练学习上下文相关的词表示。其核心创新是 Masked Language Model (MLM) 和 Next Sentence Prediction (NSP)。

---

**2. 模型架构**
**(1) 输入表示**

BERT 的输入由三部分组成：
$$
\text{Input} = \text{Token Embedding} + \text{Segment Embedding} + \text{Position Embedding}
$$
• Token Embedding：词向量（WordPiece 分词）

• Segment Embedding：区分句子（如 `[CLS]` 和 `[SEP]`）

• Position Embedding：位置编码（最大长度 512）


**(2) Transformer 层**

基于多头自注意力机制（Multi-Head Attention）：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
其中：
• $Q, K, V$ 分别是查询、键、值矩阵

• $d_k$ 是向量维度


**(3) 预训练任务**

1. Masked Language Model (MLM)  
   随机遮盖 15% 的 token 并预测：
   $$
   P(w_i | w_{1..i-1}, w_{i+1..n}) = \text{softmax}(\mathbf{W}_o \mathbf{h}_i)
   $$
   • $\mathbf{h}_i$ 是第 $i$ 个 token 的隐藏层输出

   • $\mathbf{W}_o$ 是输出权重矩阵


2. Next Sentence Prediction (NSP)  
   预测两个句子是否连续：
   $$
   P(\text{IsNext}) = \sigma(\mathbf{w}^T \mathbf{h}_{[CLS]})
   $$
   • $\mathbf{h}_{[CLS]}$ 是 `[CLS]` token 的向量

   • $\sigma$ 是 sigmoid 函数


---

**3. 关键公式**

**(1) 自注意力计算**

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)\mathbf{W}^O
$$
其中每个注意力头：
$$
\text{head}_i = \text{Attention}(Q\mathbf{W}_i^Q, K\mathbf{W}_i^K, V\mathbf{W}_i^V)
$$

**(2) 层归一化 (LayerNorm)**
$$
\text{LayerNorm}(\mathbf{x}) = \gamma \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
• $\mu, \sigma$ 是均值和方差

• $\gamma, \beta$ 是可学习参数


**(3) 位置编码 (Positional Encoding)**
$$
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{\text{model}}})
$$
$$
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})
$$
• $pos$ 是位置索引

• $i$ 是维度索引


---

**4. 与 Word2Vec 对比**
| 特性         | Word2Vec              | BERT                  |
|------------------|--------------------------|--------------------------|
| 上下文建模   | 局部窗口 $C$             | 全局双向上下文           |
| 训练目标     | 词预测                   | MLM + NSP                |
| 多义词处理   | ❌                        | ✅                        |
| 计算复杂度   | $O(d \cdot C)$           | $O(L^2 \cdot d)$         |

---

**5. 代码示例 (PyTorch)**
```python
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello world!", return_tensors="pt")
outputs = model(**inputs)

# 获取词向量
word_vectors = outputs.last_hidden_state  # [1, seq_len, 768]
```

---

